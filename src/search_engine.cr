require "redis"

# `SearchEngine` implements the logic to crawl and extract dates from a list of
# URLs in parallel. Crawling is performed using `Crawler`, and results are
# generated by `DateExtraction`.
class SearchEngine
  VERSION = "0.1.0"

  getter crawler : Crawler

  def initialize(redis_url = ENV["REDIS_URL"]?)
    @crawler = Crawler.new(redis_url)
  end

  # Crawls and extracts publication dates from a list of URLs in parallel.
  # Results are returned asynchronously as a channel of `Result`s.
  # The channel will be closed once the final result is sent.
  def crawl(urls : Array(URI)) : Channel(Result)
    channel = Channel(Result).new(10)
    atomic = Atomic.new(urls.size)

    urls.each do |url|
      spawn do
        date = nil
        error_message = nil

        crawl_time = Time.measure do
          page = @crawler.crawl(url)
          date = DateExtraction.extract_date(page)
        rescue ex
          error_message = ex.message
        end

        if error_message
          channel.send Result.new(url, error_message, crawl_time)
        else
          channel.send Result.new(url, date, crawl_time)
        end

        if atomic.sub(1) == 1
          # We are the final fiber to complete, close the channel
          # We test against 1 because #sub returns the old value of the atomic.
          channel.close
        end
      end
    end

    channel
  end

  # A result for the search engine for a single URL.
  # Contains date extracted, as well as crawl time.
  class Result
    # The URI of the crawled page. Identical to the URL provided to `#crawl`.
    getter url : URI

    # Date that the crawled page was published on, or `nil` if no date could be
    # ascertained.
    getter date : DateExtraction::Date?

    # String representation of any error which occured while parsing this page.
    #
    # If this is set, date extraction failed and `date` is `nil`.
    getter error_message : String?

    # Time taken to process this page.
    getter crawl_time : Time::Span

    def initialize(@url, @date : DateExtraction::Date?, @crawl_time)
    end

    def initialize(@url, @error_message : String, @crawl_time)
    end

    # Serializes this result to JSON.
    #
    # The `url` is represented in string form, `date`'s representation is
    # documented in `DateExtraction::Date#to_json`, and `crawl_time` is
    # represented as float seconds.
    def to_json(builder)
      {
        url:           url.to_s,
        date:          date,
        error_message: error_message,
        crawl_time:    crawl_time.to_f,
      }.to_json(builder)
    end
  end
end

require "./crawler"
require "./date_extraction"
